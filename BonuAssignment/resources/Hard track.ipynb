{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1f56c79a",
   "metadata": {},
   "source": [
    "# News Article Clustering\n",
    "\n",
    "In this track you're asked to cluster news articles into the topics. You will use a subset of [fetch_20newsgroups](https://scikit-learn.org/stable/modules/generated/sklearn.datasets.fetch_20newsgroups.html) dataset. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3cf082b3",
   "metadata": {},
   "source": [
    "## Marking criteria: \n",
    "\n",
    "- **8 points**: Implement TF-IDF for words vectorization.\n",
    "- **2 points**: Reduce dimensionality with TruncatedSVD and answer the theoretical questions about it.\n",
    "- **10 points**: Implement DBSCAN clustering algorithm."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2c3a202",
   "metadata": {},
   "source": [
    "## Baseline \n",
    "\n",
    "You can look at the baseline, or the basic solution of this task. Your assignments will be to implement TF-IDF vectorizer and DBSCAN from scratch and append dimension reduction on the token vectors to improve the results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8cfb796",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.datasets import fetch_20newsgroups\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "from sklearn.cluster import DBSCAN\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "# Data set\n",
    "# Take only 3 categories from 20\n",
    "newsgroups_train = fetch_20newsgroups(subset='train', remove=('headers', 'footers', 'quotes'),\n",
    "                                     categories = [ 'alt.atheism', 'talk.religion.misc', 'comp.graphics'])\n",
    "# Example of the sample\n",
    "print(f\"Sample:\\n{newsgroups_train.data[0]}\\n\")\n",
    "\n",
    "# Tokenization the articles\n",
    "vectorizer = TfidfVectorizer(max_df=0.5, min_df=2, stop_words='english')\n",
    "X = vectorizer.fit_transform(newsgroups_train.data)\n",
    "\n",
    "# Convert TF-IDF to pd.Dataframe for simplicity\n",
    "tfidf_tokens = vectorizer.get_feature_names_out() \n",
    "df_tfidfvect = pd.DataFrame(data = X.toarray(), columns = tfidf_tokens)\n",
    "print(f\"TF-IDF table:\\n {df_tfidfvect}\\n\")\n",
    "\n",
    "# Reduce the dimension\n",
    "svdT = TruncatedSVD(n_components=50)\n",
    "svdTFit = svdT.fit_transform(df_tfidfvect)\n",
    "\n",
    "# Clusterization\n",
    "db = DBSCAN(eps=0.5, min_samples=5).fit(svdTFit)\n",
    "skl_labels = db.labels_\n",
    "\n",
    "# See how many clusters we've got (ideally, should be 3 and some outliers marked as -1)\n",
    "skl_labels.sort()\n",
    "print(f\"Article labels {skl_labels}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c84d419",
   "metadata": {},
   "source": [
    "### 1 Words vectorization & Dimensionality reduction by TruncatedSVD [10 points]\n",
    "\n",
    "As far as you have short articles (or set of words, or **documents**) as data points, you cannot apply clusterization to them directly. Thus, you need to encode the articles somehow. In this assignment you're suggested to use TF-IDF as a tokenizer. TF-IDF (Term Frequency - Inverse Document Frequency) get the corpus of sentences and produce a matrix $T$ with shape $N$ x $M$, where $N$ - number of sentences, $M$ - number of **all** unique words in the whole corpus. The item of the matrix $T_i,j$ ($i-th$ word in $j-th$ document) is calculated as follows:\n",
    "\n",
    "$$T(i,j) = tf(i,j) * idf(i) $$\n",
    "\n",
    "where\n",
    "\n",
    "$$tf(i, j) = {n_{i,j} \\over \\sum_j n_{k,j}}$$\n",
    "\n",
    "$n_{i, j} $ - number of entrances of the word in the sentence,${\\sum_j n_ {k, j}} $- overall number of words in the document. So, it's a simple frequency of a particular word in a sentence; note it can be equal to zero if the word does not appear in the document. $idf(i) $ is equal to:\n",
    "\n",
    "$$idf(i) = log {|D| \\over |{d_k \\in D, | n_i \\in d_k}|}$$ \n",
    "\n",
    "$|D|$ - overall number of documents, and the denominator represents the number of documents, containing the word $n_i$.\n",
    "\n",
    "By the end of the day, all of your articles are encoded into the TF-IDF matrix as its row, have the same dimensionality and might be used for clustering algorithms. \n",
    "\n",
    "While we do not work with semantic analysis of the text for clusterization, such tokens would implicitly encode the structure of the documents: we can suppose that the documents from one category (e.g. sports) utilize the similar words. TF-IDF gains with higher value the words that appear a lot in the several documents, but not in all corpus. Also note that the TF-IDF matrix would be very sparse and you'll need to reduce its dimensionality. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58f73c6f",
   "metadata": {},
   "source": [
    "### 1.1 TF-IDF [8 points]\n",
    "\n",
    "In this task you're asked to **implement the formulas above** from scratch to calculate a TF-IDF matrix. \n",
    "\n",
    "You also have a subtask, which is important from the practical view: data cleaning. The articles contain a lot of punctuation, special characters and various forms of the same words that you need to handle. For example, words *clusters*, *Clusters*, *clusters? * *_clusters* without preprocessing would be considered as different, which greatly increase the number of columns in a TF-IDF matrix and reduce its descriptive power. Therefore, **before** the calculation of the metrics, it's recommended to:\n",
    "1) Remove all punctuation marks;\n",
    "2) Remove special symbols, such as `\\n` and `\\t`;\n",
    "3) Remove stop words, i.e. the most common and non-informative, such a `a`, `the`, `I`, etc. You can use `nltk` package to load them;\n",
    "4) Convert every word in the lower case.\n",
    "\n",
    "Also, before the calculation you can remove the words that are too rare in the corpus, for example, appear less than 2 times (see [TfidfVectorizer](https://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.TfidfVectorizer.html) documentation). \n",
    "\n",
    "***Do not use any additional libraries***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47a5f815",
   "metadata": {},
   "outputs": [],
   "source": [
    "import string\n",
    "import math \n",
    "import numpy as np\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "\n",
    "nltk.download('stopwords')\n",
    "stops = set(stopwords.words(\"English\"))\n",
    "    \n",
    "# Write your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f53caf9e",
   "metadata": {},
   "source": [
    "### 1.2 TruncatedSVD [2 points]\n",
    "\n",
    "Since the number of features of the resulting frame is pretty big, it would not be reasonable to clusterize them directly. However, PCA algorithm is not applicable here, because the data is too sparse (try yourself, sklearn should warn you!). Instead of that we are going to use **Truncated Singular Value Decomposition**.\n",
    "\n",
    "You can use the realization of TruncatedSVD from sklearn. You're asked to read the following articles and pages [1](https://en.wikipedia.org/wiki/Singular_value_decomposition), [2](https://langvillea.people.cofc.edu/DISSECTION-LAB/Emmie%27sLSI-SVDModule/p5module.html) and answer the questions:\n",
    "\n",
    "1) What are the ways to choose the proper number of eigenvalues in truncated SVD?\n",
    "2) In which cases truncated SVD is not appropriate?\n",
    "3) Are there any specific preprocessing steps that should be taken before applying truncated singular value decomposition to a raw text corpus?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69114990",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import TruncatedSVD\n",
    "\n",
    "# You can change n_components to another number\n",
    "svdT = TruncatedSVD(n_components=50)\n",
    "svdTFit = svdT.fit_transform(my_data_frame)\n",
    "svdTFit"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc97ec84",
   "metadata": {},
   "source": [
    "### 2. DBSCAN [10 points]\n",
    "\n",
    "Now everything is ready for clusterization. In this task you're asked to **implement** DBSCAN algorithm. We do not provide any guidelines, but you can use the lecture slides.\n",
    "\n",
    "The data set is not the easiest one for clusterization, even with such a powerful algorithm. Thus, your goal is to realize DBSCAN that divides it to at least **two clusters**, but **not more than six**, except the outliers. Print the labels as a proof. In case you have done everything you could, but still have only one cluster, choose other 3 topic from fetch_20newsgroups ([check](https://scikit-learn.org/stable/datasets/real_world.html#newsgroups-dataset) for target names). The distinct articles can be more separable. \n",
    "\n",
    "***Do not use any additional libraries***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fde2b985",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import metrics\n",
    "import numpy as np\n",
    "import math \n",
    "\n",
    "# Write your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a2c3ef0",
   "metadata": {},
   "source": [
    "### References\n",
    "\n",
    "https://scikit-learn.org/stable/modules/generated/sklearn.cluster.DBSCAN.html\n",
    "https://en.wikipedia.org/wiki/Tfâ€“idf"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
