{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1f56c79a",
   "metadata": {},
   "source": [
    "# News Article Clustering\n",
    "\n",
    "In this track you're asked to cluster news articles into the topics. You will use a subset of [fetch_20newsgroups](https://scikit-learn.org/stable/modules/generated/sklearn.datasets.fetch_20newsgroups.html) dataset. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3cf082b3",
   "metadata": {},
   "source": [
    "## Marking criteria: \n",
    "\n",
    "- **8 points**: Implement TF-IDF for words vectorization.\n",
    "- **2 points**: Reduce dimensionality with TruncatedSVD and answer the theoretical questions about it.\n",
    "- **10 points**: Implement DBSCAN clustering algorithm."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2c3a202",
   "metadata": {},
   "source": [
    "## Baseline \n",
    "\n",
    "You can look at the baseline, or the basic solution of this task. Your assignments will be to implement TF-IDF vectorized and DBSCAN from scratch and append dimension reduction on the token vectors to improve the results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e8cfb796",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-08T05:12:21.158720300Z",
     "start_time": "2023-05-08T05:12:19.235779100Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample:\n",
      "Those things,\n",
      "\twhich ye have both learned, and received,\n",
      "\tand heard, and seen in me,\n",
      "\tdo:\n",
      "\tand the God of peace shall be with you.\n",
      "\n",
      "TF-IDF table:\n",
      "        00  000   01  0184   02  0200   03   04   05  0511  ...  zlumber  zone   \n",
      "0     0.0  0.0  0.0   0.0  0.0   0.0  0.0  0.0  0.0   0.0  ...      0.0   0.0  \\\n",
      "1     0.0  0.0  0.0   0.0  0.0   0.0  0.0  0.0  0.0   0.0  ...      0.0   0.0   \n",
      "2     0.0  0.0  0.0   0.0  0.0   0.0  0.0  0.0  0.0   0.0  ...      0.0   0.0   \n",
      "3     0.0  0.0  0.0   0.0  0.0   0.0  0.0  0.0  0.0   0.0  ...      0.0   0.0   \n",
      "4     0.0  0.0  0.0   0.0  0.0   0.0  0.0  0.0  0.0   0.0  ...      0.0   0.0   \n",
      "...   ...  ...  ...   ...  ...   ...  ...  ...  ...   ...  ...      ...   ...   \n",
      "1436  0.0  0.0  0.0   0.0  0.0   0.0  0.0  0.0  0.0   0.0  ...      0.0   0.0   \n",
      "1437  0.0  0.0  0.0   0.0  0.0   0.0  0.0  0.0  0.0   0.0  ...      0.0   0.0   \n",
      "1438  0.0  0.0  0.0   0.0  0.0   0.0  0.0  0.0  0.0   0.0  ...      0.0   0.0   \n",
      "1439  0.0  0.0  0.0   0.0  0.0   0.0  0.0  0.0  0.0   0.0  ...      0.0   0.0   \n",
      "1440  0.0  0.0  0.0   0.0  0.0   0.0  0.0  0.0  0.0   0.0  ...      0.0   0.0   \n",
      "\n",
      "      zoo      zoom  zooming  zoroaster  zoroastrian  zoroastrianism   \n",
      "0     0.0  0.000000      0.0        0.0          0.0             0.0  \\\n",
      "1     0.0  0.000000      0.0        0.0          0.0             0.0   \n",
      "2     0.0  0.000000      0.0        0.0          0.0             0.0   \n",
      "3     0.0  0.000000      0.0        0.0          0.0             0.0   \n",
      "4     0.0  0.148324      0.0        0.0          0.0             0.0   \n",
      "...   ...       ...      ...        ...          ...             ...   \n",
      "1436  0.0  0.000000      0.0        0.0          0.0             0.0   \n",
      "1437  0.0  0.000000      0.0        0.0          0.0             0.0   \n",
      "1438  0.0  0.000000      0.0        0.0          0.0             0.0   \n",
      "1439  0.0  0.000000      0.0        0.0          0.0             0.0   \n",
      "1440  0.0  0.000000      0.0        0.0          0.0             0.0   \n",
      "\n",
      "      zoroastrians  zyxel  \n",
      "0              0.0    0.0  \n",
      "1              0.0    0.0  \n",
      "2              0.0    0.0  \n",
      "3              0.0    0.0  \n",
      "4              0.0    0.0  \n",
      "...            ...    ...  \n",
      "1436           0.0    0.0  \n",
      "1437           0.0    0.0  \n",
      "1438           0.0    0.0  \n",
      "1439           0.0    0.0  \n",
      "1440           0.0    0.0  \n",
      "\n",
      "[1441 rows x 8896 columns]\n",
      "\n",
      "Article labels [0 0 0 ... 1 1 1]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.datasets import fetch_20newsgroups\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "from sklearn.cluster import DBSCAN\n",
    "import pandas as pd\n",
    "\n",
    "# Data set\n",
    "# Take only 3 categories from 20\n",
    "newsgroups_train = fetch_20newsgroups(subset='train', remove=('headers', 'footers', 'quotes'),\n",
    "                                      categories=['alt.atheism', 'talk.religion.misc', 'comp.graphics'])\n",
    "# Example of the sample\n",
    "print(f\"Sample:\\n{newsgroups_train.data[0]}\\n\")\n",
    "\n",
    "# Tokenization the articles\n",
    "vectorizer = TfidfVectorizer(max_df=0.5, min_df=2, stop_words='english')\n",
    "X = vectorizer.fit_transform(newsgroups_train.data)\n",
    "\n",
    "# Convert TF-IDF to pd.Dataframe for simplicity\n",
    "tfidf_tokens = vectorizer.get_feature_names_out()\n",
    "df_tfidfvect = pd.DataFrame(data=X.toarray(), columns=tfidf_tokens)\n",
    "print(f\"TF-IDF table:\\n {df_tfidfvect}\\n\")\n",
    "\n",
    "# Reduce the dimension\n",
    "svdT = TruncatedSVD(n_components=50)\n",
    "svdTFit = svdT.fit_transform(df_tfidfvect)\n",
    "\n",
    "# Clusterization\n",
    "db = DBSCAN(eps=0.5, min_samples=5).fit(svdTFit)\n",
    "skl_labels = db.labels_\n",
    "\n",
    "# See how many clusters we've got (ideally, should be 3 and some outliers marked as -1)\n",
    "skl_labels.sort()\n",
    "print(f\"Article labels {skl_labels}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c84d419",
   "metadata": {},
   "source": [
    "### 1 Words vectorization & Dimensionality reduction by TruncatedSVD [10 points]\n",
    "\n",
    "As far as you have short articles (or set of words, or **documents**) as data points, you cannot apply clusterization to them directly. Thus, you need to encode the articles somehow. In this assignment you're suggested to use TF-IDF as a tokenizer. TF-IDF (Term Frequency - Inverse Document Frequency) get the corpus of sentences and produce a matrix $T$ with shape $N$ x $M$, where $N$ - number of sentences, $M$ - number of **all** unique words in the whole corpus. The item of the matrix $T_i,j$ ($i-th$ word in $j-th$ document) is calculated as follows:\n",
    "\n",
    "$$T(i,j) = tf(i,j) * idf(i) $$\n",
    "\n",
    "where\n",
    "\n",
    "$$tf(i, j) = {n_{i,j} \\over \\sum_j n_{k,j}}$$\n",
    "\n",
    "$n_{i, j} $ - number of entrances of the word in the sentence,${\\sum_j n_ {k, j}} $- overall number of words in the document. So, it's a simple frequency of a particular word in a sentence; note it can be equal to zero if the word does not appear in the document. $idf(i) $ is equal to:\n",
    "\n",
    "$$idf(i) = log {|D| \\over |{d_k \\in D, | n_i \\in d_k}|}$$ \n",
    "\n",
    "$|D|$ - overall number of documents, and the denominator represents the number of documents, containing the word $n_i$.\n",
    "\n",
    "By the end of the day, all of your articles are encoded into the TF-IDF matrix as its row, have the same dimensionality and might be used for clustering algorithms. \n",
    "\n",
    "While we do not work with semantic analysis of the text for clusterization, such tokens would implicitly encode the structure of the documents: we can suppose that the documents from one category (e.g. sports) utilize the similar words. TF-IDF gains with higher value the words that appear a lot in the several documents, but not in all corpus. Also note that the TF-IDF matrix would be very sparse, and you'll need to reduce its dimensionality."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58f73c6f",
   "metadata": {},
   "source": [
    "### 1.1 TF-IDF [8 points]\n",
    "\n",
    "In this task you're asked to **implement the formulas above** from scratch to calculate a TF-IDF matrix. \n",
    "\n",
    "You also have a subtask, which is important from the practical view: data cleaning. The articles contain a lot of punctuation, special characters and various forms of the same words that you need to handle. For example, words *clusters*, *Clusters*, *clusters? * *_clusters* without preprocessing would be considered as different, which greatly increase the number of columns in a TF-IDF matrix and reduce its descriptive power. Therefore, **before** the calculation of the metrics, it's recommended to:\n",
    "1) Remove all punctuation marks;\n",
    "2) Remove special symbols, such as `\\n` and `\\t`;\n",
    "3) Remove stop words, i.e. the most common and non-informative, such a `a`, `the`, `I`, etc. You can use `nltk` package to load them;\n",
    "4) Convert every word in the lower case.\n",
    "\n",
    "Also, before the calculation you can remove the words that are too rare in the corpus, for example, appear less than 2 times (see [TfidfVectorizer](https://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.TfidfVectorizer.html) documentation). \n",
    "\n",
    "***Do not use any additional libraries***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "47a5f815",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-08T05:12:49.612951900Z",
     "start_time": "2023-05-08T05:12:21.158720300Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\Admin\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": "      traditionally  withyou  notwhy  interaction  magian  authored  irix   \n0               0.0      0.0     0.0          0.0     0.0       0.0   0.0  \\\n1               0.0      0.0     0.0          0.0     0.0       0.0   0.0   \n2               0.0      0.0     0.0          0.0     0.0       0.0   0.0   \n3               0.0      0.0     0.0          0.0     0.0       0.0   0.0   \n4               0.0      0.0     0.0          0.0     0.0       0.0   0.0   \n...             ...      ...     ...          ...     ...       ...   ...   \n1436            0.0      0.0     0.0          0.0     0.0       0.0   0.0   \n1437            0.0      0.0     0.0          0.0     0.0       0.0   0.0   \n1438            0.0      0.0     0.0          0.0     0.0       0.0   0.0   \n1439            0.0      0.0     0.0          0.0     0.0       0.0   0.0   \n1440            0.0      0.0     0.0          0.0     0.0       0.0   0.0   \n\n      peculiar  affecting  evidencefor  ...   qp  womans  conclusively   \n0          0.0        0.0          0.0  ...  0.0     0.0           0.0  \\\n1          0.0        0.0          0.0  ...  0.0     0.0           0.0   \n2          0.0        0.0          0.0  ...  0.0     0.0           0.0   \n3          0.0        0.0          0.0  ...  0.0     0.0           0.0   \n4          0.0        0.0          0.0  ...  0.0     0.0           0.0   \n...        ...        ...          ...  ...  ...     ...           ...   \n1436       0.0        0.0          0.0  ...  0.0     0.0           0.0   \n1437       0.0        0.0          0.0  ...  0.0     0.0           0.0   \n1438       0.0        0.0          0.0  ...  0.0     0.0           0.0   \n1439       0.0        0.0          0.0  ...  0.0     0.0           0.0   \n1440       0.0        0.0          0.0  ...  0.0     0.0           0.0   \n\n      approximation  andfalling  isso  reinforcing  theimage  problemwith   \n0               0.0         0.0   0.0          0.0       0.0          0.0  \\\n1               0.0         0.0   0.0          0.0       0.0          0.0   \n2               0.0         0.0   0.0          0.0       0.0          0.0   \n3               0.0         0.0   0.0          0.0       0.0          0.0   \n4               0.0         0.0   0.0          0.0       0.0          0.0   \n...             ...         ...   ...          ...       ...          ...   \n1436            0.0         0.0   0.0          0.0       0.0          0.0   \n1437            0.0         0.0   0.0          0.0       0.0          0.0   \n1438            0.0         0.0   0.0          0.0       0.0          0.0   \n1439            0.0         0.0   0.0          0.0       0.0          0.0   \n1440            0.0         0.0   0.0          0.0       0.0          0.0   \n\n      entrails  \n0          0.0  \n1          0.0  \n2          0.0  \n3          0.0  \n4          0.0  \n...        ...  \n1436       0.0  \n1437       0.0  \n1438       0.0  \n1439       0.0  \n1440       0.0  \n\n[1441 rows x 10612 columns]",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>traditionally</th>\n      <th>withyou</th>\n      <th>notwhy</th>\n      <th>interaction</th>\n      <th>magian</th>\n      <th>authored</th>\n      <th>irix</th>\n      <th>peculiar</th>\n      <th>affecting</th>\n      <th>evidencefor</th>\n      <th>...</th>\n      <th>qp</th>\n      <th>womans</th>\n      <th>conclusively</th>\n      <th>approximation</th>\n      <th>andfalling</th>\n      <th>isso</th>\n      <th>reinforcing</th>\n      <th>theimage</th>\n      <th>problemwith</th>\n      <th>entrails</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>...</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>...</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>...</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>...</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>...</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>1436</th>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>...</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n    </tr>\n    <tr>\n      <th>1437</th>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>...</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n    </tr>\n    <tr>\n      <th>1438</th>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>...</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n    </tr>\n    <tr>\n      <th>1439</th>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>...</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n    </tr>\n    <tr>\n      <th>1440</th>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>...</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n    </tr>\n  </tbody>\n</table>\n<p>1441 rows × 10612 columns</p>\n</div>"
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import string\n",
    "import math\n",
    "import numpy as np\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "nltk.download('stopwords')\n",
    "stops = set(stopwords.words(\"English\"))\n",
    "\n",
    "#  load 3 subsets in the same manner as in the baseline\n",
    "newsgroups_train = fetch_20newsgroups(subset='train', remove=('headers', 'footers', 'quotes'),\n",
    "                                      categories=['alt.atheism', 'talk.religion.misc', 'comp.graphics'])\n",
    "\n",
    "\n",
    "def preprocess_text(text):\n",
    "    text = text.translate(str.maketrans(\"\", \"\", string.punctuation))\n",
    "    text = text.replace(\"\\n\", \"\").replace(\"\\t\", \"\")\n",
    "    text = text.lower()\n",
    "    words = word_tokenize(text)\n",
    "    words = [word for word in words if word not in stops]\n",
    "    preprocessed_text = \" \".join(words)\n",
    "\n",
    "    return preprocessed_text\n",
    "\n",
    "\n",
    "preprocessed_documents = [preprocess_text(document) for document in newsgroups_train.data]\n",
    "\n",
    "vocabulary = set()\n",
    "for document in preprocessed_documents:\n",
    "    words = document.split()\n",
    "    vocabulary.update(words)\n",
    "\n",
    "word_frequencies = {}\n",
    "for document in preprocessed_documents:\n",
    "    words = document.split()\n",
    "    for word in words:\n",
    "        if word not in word_frequencies:\n",
    "            word_frequencies[word] = 0\n",
    "        word_frequencies[word] += 1\n",
    "\n",
    "filtered_vocabulary = [word for word in vocabulary if word_frequencies[word] >= 2]\n",
    "\n",
    "num_documents = len(preprocessed_documents)\n",
    "\n",
    "tf_idf_matrix = np.zeros((num_documents, len(filtered_vocabulary)))\n",
    "for i, doc in enumerate(preprocessed_documents):\n",
    "    words = doc.split()\n",
    "    for j, word in enumerate(filtered_vocabulary):\n",
    "        tf = words.count(word) / len(words) if len(words) > 0 else 0\n",
    "        idf = math.log(num_documents / word_frequencies[word])\n",
    "        tf_idf = tf * idf\n",
    "        tf_idf_matrix[i, j] = tf_idf\n",
    "\n",
    "# pandas used only for final dataframe formation\n",
    "tf_idf_df = pd.DataFrame(tf_idf_matrix, columns=list(filtered_vocabulary))\n",
    "tf_idf_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f53caf9e",
   "metadata": {},
   "source": [
    "### 1.2 TruncatedSVD [2 points]\n",
    "\n",
    "Since the number of features of the resulting frame is pretty big, it would not be reasonable to clusterize them directly. However, PCA algorithm is not applicable here, because the data is too sparse (try yourself, sklearn should warn you!). Instead of that we are going to use **Truncated Singular Value Decomposition**.\n",
    "\n",
    "You can use the realization of TruncatedSVD from sklearn. You're asked to read the following articles and pages [1](https://en.wikipedia.org/wiki/Singular_value_decomposition), [2](https://langvillea.people.cofc.edu/DISSECTION-LAB/Emmie%27sLSI-SVDModule/p5module.html) and answer the questions:\n",
    "\n",
    "1) What are the ways to choose the proper number of eigenvalues in truncated SVD?\n",
    "    - Plotting singular values in decreasing order and examining point of diminishing returns can help determine number of significant eigenvalues to retain. The elbow represents drop-off point where eigenvalues become less significant.\n",
    "    - Calculating cumulative sum of explained variances of eigenvalues and choosing number of eigenvalues that explain a desired percentage of total variance.\n",
    "    - Having prior knowledge about dataset or specific problem at hand can guide the selection of number of eigenvalues. For example, if dataset is related to images and objective is to capture distinct visual features then domain knowledge can help in determining number of eigenvalues that represent those features.\n",
    "2) In which cases truncated SVD is not appropriate?\n",
    "    - Truncated SVD is specifically designed for sparse or high-dimensional data. If data is not sparse and has a low-dimensional representation traditional principal component analysis can be more suitable.\n",
    "    - Truncated SVD assumes that data is linearly independent. If data exhibits a strong linear dependency truncated SVD may not provide meaningful results. In such cases other techniques such as non-linear dimensionality reduction methods might be more appropriate.\n",
    "3) Are there any specific preprocessing steps that should be taken before applying truncated singular value decomposition to a raw text corpus?\n",
    "    - Tokenization\n",
    "    - Stop word removal\n",
    "    - Cleaning and normalization\n",
    "    - Term frequency-inverse document frequency weighting\n",
    "    - Sparse matrix representation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "69114990",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-08T05:12:49.629132Z",
     "start_time": "2023-05-08T05:12:49.614921700Z"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.decomposition import TruncatedSVD\n",
    "\n",
    "my_data = np.array([[1, 2, 3], [4, 5, 6], [7, 8, 9]])\n",
    "\n",
    "svdT = TruncatedSVD(n_components=2)\n",
    "svdTFit = svdT.fit_transform(my_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc97ec84",
   "metadata": {},
   "source": [
    "### 2. DBSCAN [10 points]\n",
    "\n",
    "Now everything is ready for clusterization. In this task you're asked to **implement** DBSCAN algorithm. We do not provide any guidelines, but you can use the lecture slides.\n",
    "\n",
    "The data set is not the easiest one for clusterization, even with such a powerful algorithm. Thus, your goal is to realize DBSCAN that divides it to at least **two clusters**, but **not more than six**, except the outliers. Print the labels as a proof. In case you have done everything you could, but still have only one cluster, choose other 3 topic from fetch_20newsgroups ([check](https://scikit-learn.org/stable/datasets/real_world.html#newsgroups-dataset) for target names). The distinct articles can be more separable. \n",
    "\n",
    "***Do not use any additional libraries***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "fde2b985",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-08T05:12:49.704160Z",
     "start_time": "2023-05-08T05:12:49.633134900Z"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn import metrics\n",
    "import numpy as np\n",
    "from math import sqrt\n",
    "\n",
    "\n",
    "def euclidean_distance(point1, point2):\n",
    "    squared_distance = np.sum(np.square(point1 - point2))\n",
    "\n",
    "    return sqrt(squared_distance)\n",
    "\n",
    "\n",
    "def get_neighbors(data, point_index, epsilon):\n",
    "    neighbors = []\n",
    "    for i, point in enumerate(data):\n",
    "        if euclidean_distance(data[point_index], point) <= epsilon:\n",
    "            neighbors.append(i)\n",
    "\n",
    "    return neighbors\n",
    "\n",
    "\n",
    "def dbscan(data, epsilon, min_points):\n",
    "    labels = np.zeros(len(data), dtype=int)\n",
    "    cluster_id = 1\n",
    "    for i, point in enumerate(data):\n",
    "        if labels[i] != 0:\n",
    "            continue\n",
    "        neighbors = get_neighbors(data, i, epsilon)\n",
    "        if len(neighbors) < min_points:\n",
    "            labels[i] = -1\n",
    "        else:\n",
    "            labels[i] = cluster_id\n",
    "            while len(neighbors) > 0:\n",
    "                current_point = neighbors[0]\n",
    "                if labels[current_point] == -1:\n",
    "                    labels[current_point] = cluster_id\n",
    "                elif labels[current_point] == 0:\n",
    "                    labels[current_point] = cluster_id\n",
    "                    current_point_neighbors = get_neighbors(data, current_point, epsilon)\n",
    "                    if len(current_point_neighbors) >= min_points:\n",
    "                        neighbors.extend(current_point_neighbors)\n",
    "                neighbors = neighbors[1:]\n",
    "            cluster_id += 1\n",
    "\n",
    "    return labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cluster Labels: [1 1 1 2 2 2]\n",
      "Silhouette Score: 0.8000313669777999\n"
     ]
    }
   ],
   "source": [
    "data = np.array([[1, 2], [2, 3], [3, 2], [8, 7], [7, 8], [8, 9]])\n",
    "epsilon = 2.0\n",
    "min_points = 2\n",
    "\n",
    "labels = dbscan(data, epsilon, min_points)\n",
    "print(\"Cluster Labels:\", labels)\n",
    "\n",
    "silhouette_score = metrics.silhouette_score(data, labels)\n",
    "print(\"Silhouette Score:\", silhouette_score)\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-05-08T05:12:49.737130800Z",
     "start_time": "2023-05-08T05:12:49.644129900Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "id": "0a2c3ef0",
   "metadata": {},
   "source": [
    "### References\n",
    "\n",
    "https://scikit-learn.org/stable/modules/generated/sklearn.cluster.DBSCAN.html\n",
    "https://en.wikipedia.org/wiki/Tf–idf"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
